import os
import tensorflow as tf
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from rdkit import Chem, RDLogger
from rdkit.Chem import AllChem, Descriptors
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# Configuration de TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
tf.config.threading.set_inter_op_parallelism_threads(0)
tf.config.threading.set_intra_op_parallelism_threads(0)

# Suppression des avertissements RDKit
RDLogger.DisableLog('rdApp.*')

def generate_descriptors(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        if mol is None:
            return None
        
        mol = Chem.AddHs(mol)
        try:
            AllChem.EmbedMolecule(mol, randomSeed=42, maxAttempts=100)
            AllChem.MMFFOptimizeMolecule(mol, maxIters=500)
        except ValueError:
            try:
                AllChem.UFFOptimizeMolecule(mol, maxIters=500)
            except ValueError:
                return None
        
        descriptors = [
            Descriptors.MolWt(mol),
            Descriptors.MolLogP(mol),
            Descriptors.NumHDonors(mol),
            Descriptors.NumHAcceptors(mol),
            Descriptors.NumRotatableBonds(mol),
            Descriptors.NumHeteroatoms(mol),
            Descriptors.FractionCSP3(mol),
            Descriptors.NumRings(mol),
            Descriptors.TPSA(mol),
            Descriptors.LabuteASA(mol)
        ]
        return descriptors
    except Exception as e:
        print(f"Error processing SMILES: {smiles}")
        print(f"Error message: {str(e)}")
        return None

def process_smiles_chunk(smiles_chunk):
    return [generate_descriptors(smiles) for smiles in smiles_chunk]

def parallel_descriptor_calculation(all_smiles, num_cores):
    chunk_size = max(1, len(all_smiles) // (num_cores * 10))
    with ProcessPoolExecutor(max_workers=num_cores) as executor:
        futures = [executor.submit(process_smiles_chunk, all_smiles[i:i+chunk_size]) 
                   for i in range(0, len(all_smiles), chunk_size)]
        
        results = []
        for future in tqdm(as_completed(futures), total=len(futures), desc="Calculating descriptors"):
            results.extend(future.result())
    
    return [r for r in results if r is not None]

def create_model(input_shape):
    model = Sequential([
        Dense(64, activation='relu', input_shape=(input_shape,)),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.3),
        Dense(16, activation='relu'),
        Dropout(0.3),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

def create_dataset(X, y, batch_size=32):
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    return dataset.shuffle(buffer_size=len(X)).batch(batch_size).prefetch(tf.data.AUTOTUNE)

def main():
    # Chargement et préparation des données
    print("Loading data...")
    train_data = pd.read_csv('/Users/johannatter/Documents/Thèse science/Prion/DeepLearning/Train_set.csv')
    test_data = pd.read_csv('/Users/johannatter/Documents/Thèse science/Prion/DeepLearning/Test_set.csv')

    # Parallélisation du calcul des descripteurs
    print("Generating descriptors...")
    all_smiles = list(train_data['Smiles']) + list(test_data['Smiles'])
    num_cores = multiprocessing.cpu_count()
    
    all_descriptors = parallel_descriptor_calculation(all_smiles, num_cores)

    # Vérification du nombre de descripteurs valides
    if len(all_descriptors) < len(all_smiles):
        print(f"Warning: {len(all_smiles) - len(all_descriptors)} molecules failed descriptor calculation")

    # Conversion en array numpy
    all_descriptors = np.array(all_descriptors)

    # Séparation des descripteurs pour les ensembles d'entraînement et de test
    train_descriptors = all_descriptors[:len(train_data)]
    test_descriptors = all_descriptors[len(train_data):]

    # Suppression des lignes avec des descripteurs manquants
    valid_train_indices = ~np.isnan(train_descriptors).any(axis=1)
    valid_test_indices = ~np.isnan(test_descriptors).any(axis=1)
    
    train_data = train_data.iloc[valid_train_indices]
    test_data = test_data.iloc[valid_test_indices]
    train_descriptors = train_descriptors[valid_train_indices]
    test_descriptors = test_descriptors[valid_test_indices]

    # Préparation des données cibles
    le = LabelEncoder()
    y_train = le.fit_transform(train_data['Activity'])
    y_test = le.transform(test_data['Activity'])

    # Normalisation des descripteurs
    scaler = StandardScaler()
    X_train = scaler.fit_transform(train_descriptors)
    X_test = scaler.transform(test_descriptors)

    # Séparation des données d'entraînement en ensemble d'entraînement et de validation
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

    # Création des datasets
    train_dataset = create_dataset(X_train, y_train)
    val_dataset = create_dataset(X_val, y_val)

    # Création du modèle
    model = create_model(X_train.shape[1])

    # Entraînement du modèle
    print("Training model...")
    early_stopping = EarlyStopping(patience=10, restore_best_weights=True)
    history = model.fit(
        train_dataset,
        epochs=100,
        validation_data=val_dataset,
        callbacks=[early_stopping],
        verbose=1
    )

    # Évaluation du modèle
    print("\nEvaluating model on test set...")
    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"Test Accuracy: {test_accuracy:.4f}")

    # Prédictions sur l'ensemble de test
    print("Making predictions on test set...")
    y_pred = model.predict(X_test)
    y_pred_classes = (y_pred > 0.5).astype(int).flatten()

    # Calcul des métriques
    accuracy = accuracy_score(y_test, y_pred_classes)
    precision = precision_score(y_test, y_pred_classes)
    recall = recall_score(y_test, y_pred_classes)
    f1 = f1_score(y_test, y_pred_classes)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Sauvegarde du modèle
    model.save('qsar_model.keras')
    print("Model saved as 'qsar_model.keras'")

if __name__ == '__main__':
    multiprocessing.set_start_method('spawn')
    main()

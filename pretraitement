import pandas as pd
import datamol as dm
from mordred import Calculator, descriptors
from tqdm import tqdm
import concurrent.futures
import pyarrow as pa
import pyarrow.parquet as pq

# Fonction pour générer les conformères et calculer les descripteurs
def process_smiles(smiles, name, activity, num_conformers=8):
    mol = dm.to_mol(smiles)
    if mol is None:
        return None
    
    # Génération des conformères
    conformers = dm.conformers.generate(mol, n_confs=num_conformers)
    
    # Calcul des descripteurs pour chaque conformère
    calc = Calculator(descriptors, ignore_3D=False)
    
    # Exclure ABCIndex des descripteurs
    calc.descriptors = [d for d in calc.descriptors if d.__class__.__name__ != 'ABCIndex']
    
    all_descs = []
    
    for i in range(conformers.GetNumConformers()):
        conf = conformers.GetConformer(i)
        descs = calc(conf.GetOwningMol())
        desc_dict = {str(d): float(v) if pd.notna(v) else None for d, v in descs.items()}
        desc_dict['Molecule Name'] = name
        desc_dict['Activity'] = activity
        desc_dict['Conformer ID'] = i
        all_descs.append(desc_dict)
    
    return all_descs

# Fonction principale
def process_csv_to_parquet(input_file, output_file, batch_size=100):
    df = pd.read_csv(input_file)
    
    # Initialiser le writer Parquet
    schema = None
    writer = None
    
    # Traiter les molécules par lots
    for i in tqdm(range(0, len(df), batch_size), desc="Processing batches"):
        batch = df.iloc[i:i+batch_size]
        all_descriptors = []
        
        # Utiliser ProcessPoolExecutor pour la parallélisation
        with concurrent.futures.ProcessPoolExecutor() as executor:
            futures = [executor.submit(process_smiles, row['Smiles'], row['Molecule Name'], row['Activity']) 
                       for _, row in batch.iterrows()]
            
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if result:
                    all_descriptors.extend(result)
        
        # Création du DataFrame pour ce lot
        batch_df = pd.DataFrame(all_descriptors)
        
        # Réorganisation des colonnes
        cols = ['Molecule Name', 'Activity', 'Conformer ID'] + [col for col in batch_df.columns if col not in ['Molecule Name', 'Activity', 'Conformer ID']]
        batch_df = batch_df[cols]
        
        # Conversion en table PyArrow
        table = pa.Table.from_pandas(batch_df)
        
        # Si c'est le premier lot, initialiser le writer
        if writer is None:
            schema = table.schema
            writer = pq.ParquetWriter(output_file, schema, compression='snappy')
        
        # Écrire le lot dans le fichier Parquet
        writer.write_table(table)
    
    # Fermer le writer
    if writer:
        writer.close()
    
    print(f"Descriptors saved to {output_file}")

if __name__ == '__main__':
    input_file = '/Users/johannatter/Documents/Thèse science/Prion/DeepLearning/Prion_DB_clean.csv'
    output_file = 'descriptors_prion.parquet'
    process_csv_to_parquet(input_file, output_file, batch_size=100)

import pandas as pd
import datamol as dm
from mordred import Calculator, descriptors
from tqdm import tqdm
import concurrent.futures
import pyarrow as pa
import pyarrow.parquet as pq
import numpy as np
import warnings

warnings.filterwarnings("ignore", category=RuntimeWarning, message="underflow encountered in exp")
warnings.filterwarnings("ignore", category=RuntimeWarning, message="overflow encountered in reduce")

def safe_calculate(func):
    try:
        return func()
    except (OverflowError, ValueError, ZeroDivisionError):
        return np.nan

def process_smiles(smiles, name, activity):
    mol = dm.to_mol(smiles)
    if mol is None:
        return None
    
    calc = Calculator(descriptors, ignore_3D=False)
    calc.descriptors = [d for d in calc.descriptors if d.__class__.__name__ != 'ABCIndex']
    
    descs = calc(mol)
    desc_dict = {str(d): safe_calculate(lambda x=v: float(x)) for d, v in descs.items()}
    desc_dict['Molecule Name'] = name
    desc_dict['Activity'] = activity
    
    return desc_dict

def process_csv_to_parquet(input_file, output_file, batch_size=100):
    df = pd.read_csv(input_file)
    
    schema = None
    writer = None
    
    for i in tqdm(range(0, len(df), batch_size), desc="Processing batches"):
        batch = df.iloc[i:i+batch_size]
        all_descriptors = []
        
        with concurrent.futures.ProcessPoolExecutor() as executor:
            futures = [executor.submit(process_smiles, row['Smiles'], row['Molecule Name'], row['Activity']) 
                       for _, row in batch.iterrows()]
            
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if result:
                    all_descriptors.append(result)
        
        batch_df = pd.DataFrame(all_descriptors)
        
        if writer is None:
            schema = pa.Schema.from_pandas(batch_df)
            writer = pq.ParquetWriter(output_file, schema, compression='snappy')
        
        table = pa.Table.from_pandas(batch_df, schema=schema)
        writer.write_table(table)
    
    if writer:
        writer.close()
    
    print(f"Descriptors saved to {output_file}")

if __name__ == '__main__':
    input_file = '/Users/johannatter/Documents/Th√®se science/Prion/DeepLearning/Prion_DB_clean.csv'
    output_file = 'descriptors_prion.parquet'
    process_csv_to_parquet(input_file, output_file, batch_size=100)
